{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyM7SigLMfHLBCnHUMaJbGb6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"USO8Zx1_azwU","executionInfo":{"status":"ok","timestamp":1638806402859,"user_tz":-330,"elapsed":638,"user":{"displayName":"Arvind Ratnu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5vgsh7xK0jXZlYrt4AIhkOiH4_nCvwbAt4DiB8Q=s64","userId":"06522329510023175000"}}},"source":["import pandas as pd\n","import numpy as np\n","import lightgbm as lgb  "],"execution_count":118,"outputs":[]},{"cell_type":"code","metadata":{"id":"rSiSt3wtm24N","executionInfo":{"status":"ok","timestamp":1638788949767,"user_tz":-330,"elapsed":428,"user":{"displayName":"Arvind Ratnu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5vgsh7xK0jXZlYrt4AIhkOiH4_nCvwbAt4DiB8Q=s64","userId":"06522329510023175000"}}},"source":["def final_fun_1(X):\n","\n","  store_data = pd.read_csv('train.csv', parse_dates=['date'], index_col=['date'])\n","  test_data = X\n","\n","  store_data['day'] = store_data.index.day\n","  store_data['month'] = store_data.index.month\n","  store_data['year'] = store_data.index.year\n","  store_data['dayofweek'] = store_data.index.dayofweek\n","\n","  test_data['day'] = test_data.index.day\n","  test_data['month'] = test_data.index.month\n","  test_data['year'] = test_data.index.year\n","  test_data['dayofweek'] = test_data.index.dayofweek\n","\n","  #Overall average sale value\n","  average = store_data.sales.mean()\n","\n","  # Pivot Table on year index\n","  year_pivot_table = pd.pivot_table(store_data, index='year', values='sales', aggfunc=np.mean) / average\n","\n","  #approximate quardatic function\n","  store_years = np.arange(2013, 2019)\n","  weight = np.exp((store_years - 2018) / 10)[:-1]\n","  store_annual_growth = np.poly1d(np.polyfit(store_years[:-1], year_pivot_table.values.squeeze(), 2, w=weight))\n","\n","  # Pivot Table on Day of Week index\n","  dayofweek_pivot_table = pd.pivot_table(store_data, index='dayofweek', columns='item', values='sales', aggfunc=np.mean)\n","\n","  # Pivot Table on Month index\n","  month_pivot_table = pd.pivot_table(store_data, index='month', values='sales', aggfunc=np.mean) / average\n","\n","  # Pivot Table on Store index\n","  store_pivot_table = pd.pivot_table(store_data, index='store', values='sales', aggfunc=np.mean) / average\n","\n","  #Calculating 2018 sales values using pivot table values(Day of Week,Monthly and Store) and multiplying with 2018 growth rate.\n","  sales_pred_2018 = []\n","  for _, test_row in test_data.iterrows():\n","    dayofweek_row, month_row= test_row.name.dayofweek, test_row.name.month\n","    item_row, store_id_row = test_row['item'], test_row['store']\n","\n","    #Calculating dayofweek,monthly and store value\n","    dayofweek_value = dayofweek_pivot_table.at[dayofweek_row, item_row]\n","    month_value = month_pivot_table.at[month_row, 'sales']\n","    sales_value =  store_pivot_table.at[store_id_row, 'sales']\n","\n","    #muliplying all previous values\n","    final_product = dayofweek_value * month_value * sales_value\n","    store_annual_growth_value = store_annual_growth(2018)\n","\n","    #muliplying with 2018 growth value\n","    sales_pred_2018.append(int(np.round(final_product * store_annual_growth_value, 0)))\n","\n","  test_data['sales'] = sales_pred_2018\n","  store = pd.concat([store_data, test_data], sort=False)\n","  store.reset_index(inplace=True)\n","\n","  store['dayofyear'] = store.date.dt.dayofyear\n","  store['weekofyear'] = store.date.dt.weekofyear\n","  store['weekend_yes'] = store.date.dt.weekday // 4\n","  store['month_start_yes'] = store.date.dt.is_month_start.astype(int)\n","  store['month_end_yes'] = store.date.dt.is_month_end.astype(int)\n","  store['quarter'] = store.date.dt.quarter\n","  store['weekofmonth'] = store['weekofyear'].values // 4.35                                                                                                                                                                               \n","  store['mon_yes'] = np.where(store['dayofweek'] == 0, 1, 0)                                                                                            \n","  store['tue_yes'] = np.where(store['dayofweek'] == 1, 1, 0)                                                                                         \n","  store['wed_yes'] = np.where(store['dayofweek'] == 2, 1, 0)                                                                                         \n","  store['thu_yes'] = np.where(store['dayofweek'] == 3, 1, 0)                                                                                         \n","  store['fri_yes'] = np.where(store['dayofweek'] == 4, 1, 0)                                                                                         \n","  store['sat_yes'] = np.where(store['dayofweek'] == 5, 1, 0)                                                                                         \n","  store['sun_yes'] = np.where(store['dayofweek'] == 6, 1, 0) \n","\n","  exp_time_features = ['dayofweek', 'weekofmonth', 'weekofyear', 'month', 'quarter', 'weekend_yes'] \n","  for exp_item in exp_time_features:\n","    expanding_store = store.groupby(['store', 'item', exp_item])['sales'].expanding().mean().bfill().reset_index()\n","    expanding_store.columns = ['store', 'item', exp_item, 'exp_index', 'exp_'+exp_item]\n","    expanding_store = expanding_store.sort_values(by=['item', 'store', 'exp_index'])\n","    store['exp_'+exp_item] = expanding_store['exp_'+exp_item].values\n","\n","  store.sort_values(by=['item', 'store', 'date'], axis=0, inplace=True)\n","\n","  #Adding Lag values as feature\n","  l = [8,15,22,29,30,31,38,61,67,73,91, 98, 105, 112, 180, 270, 365, 546, 728]                                                                                                                                                                                                                      \n","  for var_l in l:                                                                                                                          \n","    store['l_' + str(var_l)] = store.groupby([\"item\", \"store\"])['sales'].transform(lambda y: y.shift(var_l)) + np.random.normal(scale=0.01, size=(len(store),))  \n","\n","  #Adding Rolling Mean values as feature\n","  r = [8,15,22,29,30,31,38,61,67,73,91, 98, 105, 112, 180, 270, 365, 546, 728]                                                                                                                                                                                                                                                                                                                       \n","  for var_r in r:                                                                                                                    \n","    store['r_' + str(var_r)] = store.groupby([\"item\", \"store\"])['sales'].transform(lambda y: y.shift(1).rolling(window=var_r, min_periods=8, win_type=\"triang\").mean()) + np.random.normal(scale=0.01, size=(len(store),)) \n","\n","  # #Adding Exponentially Mean values as feature\n","\n","  ewm_a = [0.95, 0.9, 0.8, 0.7, 0.5,.4,.3,.2,.1]                                             \n","  ewm_l = [8,15,22,29,30,31,38,61,67,73,91, 98, 105, 112, 180, 270, 365, 546, 728]\n","                                                                                                          \n","  for var_a in ewm_a:                                                                                                                      \n","    for var_l in ewm_l:                                                                                                                      \n","      store['ewm_a_' + str(var_a) + \"_l_\" + str(var_l)] = store.groupby([\"item\", \"store\"])['sales'].transform(lambda y: y.shift(var_l).ewm(alpha=var_a).mean()) \n","\n","  store_encoding = pd.get_dummies(store[['store', 'item', 'dayofweek', 'month']], columns=['store', 'item', 'dayofweek', 'month'], dummy_na=True)  \n","  store_final = pd.concat([store, store_encoding], axis=1)                                                                                                          \n","\n","  # changing to log scale                                                                                                          \n","  store_final['sales'] = np.log1p(store_final[\"sales\"].values)\n","\n","  store_lgbm_columns = [column for column in store_final.columns if column not in ['date', 'id', 'sales', 'year']]\n","\n","  iteration = 15000\n","                                                                                                       \n","  store_lgbm_parms = {                                                                                                                            \n","          'nthread': -1,\n","          'metric': 'mae',\n","          'boosting_type': 'gbdt',    \n","          'max_depth': 7,\n","          'num_leaves': 28,   \n","          'task': 'train',                                                                                                                      \n","          'objective': 'regression_l1',                                                                                                         \n","          'learning_rate': 0.05,                                                                                                                \n","          'feature_fraction': 0.9,                                                                                                              \n","          'bagging_fraction': 0.8,                                                                                                              \n","          'bagging_freq': 5,                                                                                                                    \n","          'lambda_l1': 0.06,                                                                                                                    \n","          'lambda_l2': 0.05,                                                                                                                    \n","          'verbose': -1,     } \n","\n","  X = store_final[store_lgbm_columns] \n","  Y = store_final['sales']\n","\n","  store_lgbm = store_final.loc[store_final.id.notnull()]                                                                                                                \n","  test = store_lgbm[store_lgbm_columns] \n","                                                                                                                                                                                                                                                                                                                   \n","  store_lgbm_dataset = lgb.Dataset(data=X, label=Y, feature_name=store_lgbm_columns)                                                                \n","  store_lgbm_model = lgb.train(store_lgbm_parms, store_lgbm_dataset, num_boost_round=iteration)\n","\n","  #making predictions\n","  store_lgbm_preds = store_lgbm_model.predict(test, num_iteration=iteration)  \n","\n","  store_lgbm_preds_sales = np.round(np.expm1(store_lgbm_preds),0)\n","  return store_lgbm_preds_sales"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVyS7VzRfdnt","executionInfo":{"status":"ok","timestamp":1638806693183,"user_tz":-330,"elapsed":684,"user":{"displayName":"Arvind Ratnu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5vgsh7xK0jXZlYrt4AIhkOiH4_nCvwbAt4DiB8Q=s64","userId":"06522329510023175000"}}},"source":["def final_fun_2(X,Y):\n","\n","  store_data = pd.read_csv('train.csv', parse_dates=['date'], index_col=['date'])\n","  test_data = X\n","  target_data = Y\n","\n","  store_data['day'] = store_data.index.day\n","  store_data['month'] = store_data.index.month\n","  store_data['year'] = store_data.index.year\n","  store_data['dayofweek'] = store_data.index.dayofweek\n","\n","  test_data['day'] = test_data.index.day\n","  test_data['month'] = test_data.index.month\n","  test_data['year'] = test_data.index.year\n","  test_data['dayofweek'] = test_data.index.dayofweek\n","\n","  #Overall average sale value\n","  average = store_data.sales.mean()\n","\n","  # Pivot Table on year index\n","  year_pivot_table = pd.pivot_table(store_data, index='year', values='sales', aggfunc=np.mean) / average\n","\n","  #approximate quardatic function\n","  store_years = np.arange(2013, 2019)\n","  weight = np.exp((store_years - 2018) / 10)[:-1]\n","  store_annual_growth = np.poly1d(np.polyfit(store_years[:-1], year_pivot_table.values.squeeze(), 2, w=weight))\n","\n","  # Pivot Table on Day of Week index\n","  dayofweek_pivot_table = pd.pivot_table(store_data, index='dayofweek', columns='item', values='sales', aggfunc=np.mean)\n","\n","  # Pivot Table on Month index\n","  month_pivot_table = pd.pivot_table(store_data, index='month', values='sales', aggfunc=np.mean) / average\n","\n","  # Pivot Table on Store index\n","  store_pivot_table = pd.pivot_table(store_data, index='store', values='sales', aggfunc=np.mean) / average\n","\n","  #Calculating 2018 sales values using pivot table values(Day of Week,Monthly and Store) and multiplying with 2018 growth rate.\n","  sales_pred_2018 = []\n","  for _, test_row in test_data.iterrows():\n","    dayofweek_row, month_row= test_row.name.dayofweek, test_row.name.month\n","    item_row, store_id_row = test_row['item'], test_row['store']\n","\n","    #Calculating dayofweek,monthly and store value\n","    dayofweek_value = dayofweek_pivot_table.at[dayofweek_row, item_row]\n","    month_value = month_pivot_table.at[month_row, 'sales']\n","    sales_value =  store_pivot_table.at[store_id_row, 'sales']\n","\n","    #muliplying all previous values\n","    final_product = dayofweek_value * month_value * sales_value\n","    store_annual_growth_value = store_annual_growth(2018)\n","\n","    #muliplying with 2018 growth value\n","    sales_pred_2018.append(int(np.round(final_product * store_annual_growth_value, 0)))\n","\n","  test_data['sales'] = sales_pred_2018\n","  store = pd.concat([store_data, test_data], sort=False)\n","  store.reset_index(inplace=True)\n","\n","  store['dayofyear'] = store.date.dt.dayofyear\n","  store['weekofyear'] = store.date.dt.weekofyear\n","  store['weekend_yes'] = store.date.dt.weekday // 4\n","  store['month_start_yes'] = store.date.dt.is_month_start.astype(int)\n","  store['month_end_yes'] = store.date.dt.is_month_end.astype(int)\n","  store['quarter'] = store.date.dt.quarter\n","  store['weekofmonth'] = store['weekofyear'].values // 4.35                                                                                                                                                                               \n","  store['mon_yes'] = np.where(store['dayofweek'] == 0, 1, 0)                                                                                            \n","  store['tue_yes'] = np.where(store['dayofweek'] == 1, 1, 0)                                                                                         \n","  store['wed_yes'] = np.where(store['dayofweek'] == 2, 1, 0)                                                                                         \n","  store['thu_yes'] = np.where(store['dayofweek'] == 3, 1, 0)                                                                                         \n","  store['fri_yes'] = np.where(store['dayofweek'] == 4, 1, 0)                                                                                         \n","  store['sat_yes'] = np.where(store['dayofweek'] == 5, 1, 0)                                                                                         \n","  store['sun_yes'] = np.where(store['dayofweek'] == 6, 1, 0) \n","\n","  exp_time_features = ['dayofweek', 'weekofmonth', 'weekofyear', 'month', 'quarter', 'weekend_yes'] \n","  for exp_item in exp_time_features:\n","    expanding_store = store.groupby(['store', 'item', exp_item])['sales'].expanding().mean().bfill().reset_index()\n","    expanding_store.columns = ['store', 'item', exp_item, 'exp_index', 'exp_'+exp_item]\n","    expanding_store = expanding_store.sort_values(by=['item', 'store', 'exp_index'])\n","    store['exp_'+exp_item] = expanding_store['exp_'+exp_item].values\n","\n","  store.sort_values(by=['item', 'store', 'date'], axis=0, inplace=True)\n","\n","  #Adding Lag values as feature\n","  l = [8,15,22,29,30,31,38,61,67,73,91, 98, 105, 112, 180, 270, 365, 546, 728]                                                                                                                                                                                                                      \n","  for var_l in l:                                                                                                                          \n","    store['l_' + str(var_l)] = store.groupby([\"item\", \"store\"])['sales'].transform(lambda y: y.shift(var_l)) + np.random.normal(scale=0.01, size=(len(store),))  \n","\n","  #Adding Rolling Mean values as feature\n","  r = [8,15,22,29,30,31,38,61,67,73,91, 98, 105, 112, 180, 270, 365, 546, 728]                                                                                                                                                                                                                                                                                                                       \n","  for var_r in r:                                                                                                                    \n","    store['r_' + str(var_r)] = store.groupby([\"item\", \"store\"])['sales'].transform(lambda y: y.shift(1).rolling(window=var_r, min_periods=8, win_type=\"triang\").mean()) + np.random.normal(scale=0.01, size=(len(store),)) \n","\n","  # #Adding Exponentially Mean values as feature\n","\n","  ewm_a = [0.95, 0.9, 0.8, 0.7, 0.5,.4,.3,.2,.1]                                             \n","  ewm_l = [8,15,22,29,30,31,38,61,67,73,91, 98, 105, 112, 180, 270, 365, 546, 728]\n","                                                                                                          \n","  for var_a in ewm_a:                                                                                                                      \n","    for var_l in ewm_l:                                                                                                                      \n","      store['ewm_a_' + str(var_a) + \"_l_\" + str(var_l)] = store.groupby([\"item\", \"store\"])['sales'].transform(lambda y: y.shift(var_l).ewm(alpha=var_a).mean()) \n","\n","  store_encoding = pd.get_dummies(store[['store', 'item', 'dayofweek', 'month']], columns=['store', 'item', 'dayofweek', 'month'], dummy_na=True)  \n","  store_final = pd.concat([store, store_encoding], axis=1)                                                                                                          \n","\n","  # changing to log scale                                                                                                          \n","  store_final['sales'] = np.log1p(store_final[\"sales\"].values)\n","\n","  store_lgbm_columns = [column for column in store_final.columns if column not in ['date', 'id', 'sales', 'year']]\n","\n","  iteration = 15000\n","                                                                                                       \n","  store_lgbm_parms = {                                                                                                                            \n","          'nthread': -1,\n","          'metric': 'mae',\n","          'boosting_type': 'gbdt',    \n","          'max_depth': 7,\n","          'num_leaves': 28,   \n","          'task': 'train',                                                                                                                      \n","          'objective': 'regression_l1',                                                                                                         \n","          'learning_rate': 0.05,                                                                                                                \n","          'feature_fraction': 0.9,                                                                                                              \n","          'bagging_fraction': 0.8,                                                                                                              \n","          'bagging_freq': 5,                                                                                                                    \n","          'lambda_l1': 0.06,                                                                                                                    \n","          'lambda_l2': 0.05,                                                                                                                    \n","          'verbose': -1,     } \n","\n","  X = store_final[store_lgbm_columns] \n","  Y = store_final['sales']\n","\n","  store_lgbm = store_final.loc[store_final.id.notnull()]                                                                                                                \n","  test = store_lgbm[store_lgbm_columns] \n","                                                                                                                                                                                                                                                                                                                   \n","  store_lgbm_dataset = lgb.Dataset(data=X, label=Y, feature_name=store_lgbm_columns)                                                                \n","  store_lgbm_model = lgb.train(store_lgbm_parms, store_lgbm_dataset, num_boost_round=iteration)\n","\n","  #making predictions\n","  store_lgbm_preds = store_lgbm_model.predict(test, num_iteration=iteration)  \n","\n","  store_lgbm_preds_sales = np.round(np.expm1(store_lgbm_preds),0)\n","\n","  pred_length = len(store_lgbm_preds_sales)\n","  pred_smape_masked = ~((store_lgbm_preds_sales == 0) & (target_data == 0))\n","  store_lgbm_preds_sales, target_data = store_lgbm_preds_sales[pred_smape_masked], target_data[pred_smape_masked]\n","  pred_smape_num = np.abs(store_lgbm_preds_sales - target_data)\n","  pred_smape_den = np.abs(store_lgbm_preds_sales) + np.abs(target_data)\n","  pred_smape = (200 * np.sum(pred_smape_num / pred_smape_den)) / pred_length\n","  return pred_smape"],"execution_count":119,"outputs":[]}]}